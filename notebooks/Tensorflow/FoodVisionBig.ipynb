{"cells":[{"cell_type":"markdown","metadata":{"id":"oeyYY5EwShSD"},"source":["# Food Vision Big"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QgfOFAVfTWjf"},"outputs":[],"source":["import tensorflow as tf\n","import tensorflow_datasets as tfds\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1395,"status":"ok","timestamp":1703498743050,"user":{"displayName":"Danse Macabre","userId":"12265555024692008343"},"user_tz":480},"id":"ZR-6-gBQUEas","outputId":"e36db9cd-4857-4b50-9647-25b112ccc64e"},"outputs":[{"name":"stdout","output_type":"stream","text":["--2023-12-25 10:05:40--  https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 10246 (10K) [text/plain]\n","Saving to: ‘helper_functions.py’\n","\n","helper_functions.py 100%[===================>]  10.01K  --.-KB/s    in 0s      \n","\n","2023-12-25 10:05:41 (56.9 MB/s) - ‘helper_functions.py’ saved [10246/10246]\n","\n"]}],"source":["# Get helper functions file\n","!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py\n","from helper_functions import create_tensorboard_callback, plot_loss_curves, compare_historys"]},{"cell_type":"markdown","metadata":{"id":"yXbKZM-CStir"},"source":["## Loading the data, exploring it and preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2029,"status":"ok","timestamp":1703155112761,"user":{"displayName":"Danse Macabre","userId":"12265555024692008343"},"user_tz":480},"id":"7Avd2VmLUdJx","outputId":"b0c34e47-b278-4795-c40b-e11a4ca32c68"},"outputs":[{"name":"stdout","output_type":"stream","text":["True\n"]}],"source":["# Is the dataset that we're looking for available?\n","datasets_names = tfds.list_builders()\n","print(\"food101\" in datasets_names)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1895,"status":"ok","timestamp":1703499540892,"user":{"displayName":"Danse Macabre","userId":"12265555024692008343"},"user_tz":480},"id":"JunnCsUcUKdi","outputId":"edf02ac6-6507-4cad-9003-8413e2afa8fa"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["' split = Not all datasets have train, validation and test, this one has only training and validation.\\n    as_supervised = download data in tuple format (sample, label), e.g. (image, label)\\n    with_info = include dataset metadata? (ds_info)\\n'"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["(train_data, test_data), ds_info = tfds.load(name=\"food101\",\n","                                             split=[\"train\", \"validation\"],\n","                                             shuffle_files=True,\n","                                             as_supervised=True,\n","                                             with_info=True)\n","\n","\"\"\" split = Not all datasets have train, validation and test, this one has only training and validation.\n","    as_supervised = download data in tuple format (sample, label), e.g. (image, label)\n","    with_info = include dataset metadata? (ds_info)\n","\"\"\""]},{"cell_type":"code","execution_count":12,"metadata":{"id":"c9m57kGmZbCg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1703499781671,"user_tz":480,"elapsed":453,"user":{"displayName":"Danse Macabre","userId":"12265555024692008343"}},"outputId":"8bd35f50-5030-4410-a5be-0d2b380ed0aa"},"outputs":[{"output_type":"stream","name":"stdout","text":["FeaturesDict({\n","    'image': Image(shape=(None, None, 3), dtype=uint8),\n","    'label': ClassLabel(shape=(), dtype=int64, num_classes=101),\n","})\n"]}],"source":["print(ds_info.features)\n","\n","# Get class names\n","class_names = ds_info.features[\"label\"].names"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2531,"status":"error","timestamp":1703499612302,"user":{"displayName":"Danse Macabre","userId":"12265555024692008343"},"user_tz":480},"id":"Jnh9eDFbWwig","outputId":"0a7d666b-d456-4b75-ba8b-454c551dfd77"},"outputs":[{"ename":"NameError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-fa522fee95f1>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m   print(f\"\"\" Image shape: {image.shape} Image dtype: {image.dtype}\n\u001b[1;32m      7\u001b[0m         \u001b[0mTarget\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mFood101\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m \u001b[0mform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         Class name (str form): {class_names[label.numpy()]}\"\"\")\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'class_names' is not defined"]}],"source":["# Let's look at how this data is constructed, to know if we need to change anything\n","train_sample = train_data.take(1)\n","\n","# Output info about our training sample\n","for image, label in train_sample:\n","  print(f\"\"\" Image shape: {image.shape} Image dtype: {image.dtype}\n","        Target class from Food101 (tensor form): {label}\n","        Class name (str form): {class_names[label.numpy()]}\"\"\")\n"]},{"cell_type":"markdown","metadata":{"id":"RPN2jDaEaWWd"},"source":["All right, we see here that there are some things we need to correct, the first is that the shapes differ from image to image, we need to stay consistent and create the tensors to be of the same size.\n","\n","The second thing that we notice is that the images are not in float format, but\n","rather int, we can't have that (since we'll use efficientnet we don't need to\n","worry about scalling the pixel-values of the image to 0-1 values, but in others we might).\n","\n","The third is that the labels are not one-hot encoded so we might need to use a different loss function when creating the model.\n","\n","**We'll solve these problems with a preprocessing function.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tBjTx7uvbkEW"},"outputs":[],"source":["# Make a function for preprocessing images\n","def preprocess_img(image, label, img_shape=224):\n","  \"\"\"\n","  Converts image datatype from 'uint8' -> 'float32' and reshapes image to\n","  [img_shape, img_shape, color_channels]\n","  \"\"\"\n","  image = tf.image.resize(image, [img_shape, img_shape]) # reshape to img_shape\n","  return tf.cast(image, tf.float32), label # return (float32_image, label) tuple"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":311,"status":"ok","timestamp":1703156158728,"user":{"displayName":"Danse Macabre","userId":"12265555024692008343"},"user_tz":480},"id":"s-ctat8386lH","outputId":"b798c5c0-ec9c-4d82-9d3e-1a0468e10c9f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Image after preprocessing:\n"," [[[25.244898   12.244898    4.7295923 ]\n","  [32.40816    19.408163   11.408163  ]\n","  [32.719387   19.719387   10.719387  ]\n","  ...\n","  [39.005093   20.994862    3.4999783 ]\n","  [36.31123    19.31123     2.1173685 ]\n","  [36.413277   20.000034    4.000035  ]]\n","\n"," [[19.897957    6.897958    0.18367267]\n","  [28.943878   15.943878    7.9438777 ]\n","  [22.112244    9.112244    1.1122437 ]\n","  ...\n","  [48.89794    27.015284    6.1428356 ]\n","  [44.642845   23.852028    3.0714417 ]\n","  [42.7602     21.954084    3.0714283 ]]]...,\n","Shape: (224, 224, 3),\n","Datatype: {preprocessed_img.dtype}\n"]}],"source":["# Preprocess a single sample image and check the outputs\n","preprocessed_img = preprocess_img(image, label)[0]\n","print(f\"Image after preprocessing:\\n {preprocessed_img[:2]}...,\\nShape: {preprocessed_img.shape},\" +\n","      \"\\nDatatype: {preprocessed_img.dtype}\")"]},{"cell_type":"markdown","metadata":{"id":"53bjvfzzcKgj"},"source":["## Batching and preparing the dataset\n","\n","For loading data in the most performant way possible, see the TensorFlow docuemntation on Better performance with the tf.data API (https://www.tensorflow.org/guide/data_performance)."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16,"status":"ok","timestamp":1703499629500,"user":{"displayName":"Danse Macabre","userId":"12265555024692008343"},"user_tz":480},"id":"VykEzU5ib7XE","outputId":"66be6805-a1f7-419d-9117-8a26390fe874"},"outputs":[{"data":{"text/plain":["(<_PrefetchDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int64, name=None))>,\n"," <_PrefetchDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int64, name=None))>)"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["# Map preprocessing function to training data (and paralellize)\n","train_data = train_data.map(map_func=preprocess_img, num_parallel_calls=tf.data.AUTOTUNE)\n","\n","# Shuffle train_data and turn it into batches and prefetch it (load it faster)\n","train_data = train_data.shuffle(buffer_size=1000).batch(batch_size=32).prefetch(buffer_size=tf.data.AUTOTUNE)\n","\n","# Map prepreprocessing function to test data\n","test_data = test_data.map(preprocess_img, num_parallel_calls=tf.data.AUTOTUNE)\n","\n","# Turn test data into batches (don't need to shuffle)\n","test_data = test_data.batch(32).prefetch(tf.data.AUTOTUNE)\n","\n","\"\"\"Note: Extra: cache() - caches elements in a target dataset, saving loading time\n","(will only if your dataset is small enough to fit in memory, standard Colab instances\n","only have 12GB of memory)\"\"\"\n","\n","train_data, test_data"]},{"cell_type":"markdown","metadata":{"id":"ia-O1SXbdpAW"},"source":["## Building Feature Extraction model with Mixed Precision Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7yK0p21DduKx"},"outputs":[],"source":["# Callbacks - Create ModelCheckpoint callback to save model's progress\n","checkpoint_path = \"model_checkpoints/cp.ckpt\" # saving weights requires \".ckpt\" extension\n","model_checkpoint = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,\n","                                                      monitor=\"val_accuracy\", # save the model weights with best validation accuracy\n","                                                      save_best_only=True, # only save the best weights\n","                                                      save_weights_only=True,\n","                                                      verbose=0) # don't print out whether or not model is being saved\n","\n","# Setting mixed precision\n","tf.keras.mixed_precision.set_global_policy(policy=\"mixed_float16\")"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"kqR7ombl_RyJ","executionInfo":{"status":"ok","timestamp":1703499791221,"user_tz":480,"elapsed":3641,"user":{"displayName":"Danse Macabre","userId":"12265555024692008343"}}},"outputs":[],"source":["base_model = tf.keras.applications.efficientnet_v2.EfficientNetV2B0(include_top=False)\n","base_model.trainable = False;\n","\n","inputs = tf.keras.layers.Input(shape=(224, 224, 3), name=\"input_layer\")\n","\n","x = base_model(inputs, training=False) # Model in inference type mode only.\n","x = tf.keras.layers.GlobalAveragePooling2D(name=\"pooling_layer\")(x)\n","x = tf.keras.layers.Dense(len(class_names))(x) # We want 1 output neuron per class\n","\n","#Mixed precision requires dtype=float32\n","outputs = tf.keras.layers.Activation(\"softmax\", dtype=tf.float32, name=\"softmax_float32\")(x)\n","\n","model = tf.keras.Model(inputs, outputs)\n","\n","# Use sparse_categorical_crossentropy when labels are *not* one-hot encoded\n","model.compile(loss=\"sparse_categorical_crossentropy\",\n","              optimizer=tf.keras.optimizers.Adam(),\n","              metrics=[\"accuracy\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23,"status":"ok","timestamp":1703332865184,"user":{"displayName":"Danse Macabre","userId":"12265555024692008343"},"user_tz":480},"id":"PEGVMpagFOix","outputId":"2cfaabd8-d1b7-4202-a413-d43e5b5a5ca8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"model\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_layer (InputLayer)    [(None, 224, 224, 3)]     0         \n","                                                                 \n"," efficientnetv2-b0 (Functio  (None, None, None, 1280   5919312   \n"," nal)                        )                                   \n","                                                                 \n"," pooling_layer (GlobalMaxPo  (None, 1280)              0         \n"," oling2D)                                                        \n","                                                                 \n"," dense (Dense)               (None, 101)               129381    \n","                                                                 \n"," softmax_float32 (Activatio  (None, 101)               0         \n"," n)                                                              \n","                                                                 \n","=================================================================\n","Total params: 6048693 (23.07 MB)\n","Trainable params: 129381 (505.39 KB)\n","Non-trainable params: 5919312 (22.58 MB)\n","_________________________________________________________________\n"]}],"source":["model.summary()"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"UeimaQGMG7CY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1703500352810,"user_tz":480,"elapsed":431613,"user":{"displayName":"Danse Macabre","userId":"12265555024692008343"}},"outputId":"007655c9-deb6-4c53-8538-14b9d0047b53"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/3\n","2368/2368 [==============================] - 193s 75ms/step - loss: 1.7475 - accuracy: 0.5760 - val_loss: 1.1481 - val_accuracy: 0.6968\n","Epoch 2/3\n","2368/2368 [==============================] - 174s 73ms/step - loss: 1.2120 - accuracy: 0.6866 - val_loss: 1.0435 - val_accuracy: 0.7161\n","Epoch 3/3\n","2368/2368 [==============================] - 171s 71ms/step - loss: 1.0637 - accuracy: 0.7226 - val_loss: 0.9982 - val_accuracy: 0.7309\n"]}],"source":["history1 = model.fit(train_data,\n","                     epochs=3,\n","                     steps_per_epoch=len(train_data),\n","                     validation_data=test_data,\n","                     validation_steps=int(0.15 * len(test_data)),\n","                     callbacks=[model_checkpoint])"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"3BCpevaRK7ku","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1703500404617,"user_tz":480,"elapsed":51809,"user":{"displayName":"Danse Macabre","userId":"12265555024692008343"}},"outputId":"fdfc2a08-501b-477b-8b7b-5bf572dfd086"},"outputs":[{"output_type":"stream","name":"stdout","text":["790/790 [==============================] - 52s 65ms/step - loss: 1.0027 - accuracy: 0.7259\n"]},{"output_type":"execute_result","data":{"text/plain":["[1.002677321434021, 0.7258613705635071]"]},"metadata":{},"execution_count":15}],"source":["model.evaluate(test_data)"]},{"cell_type":"markdown","metadata":{"id":"hctSNfTfTtqO"},"source":["## Saving model to file"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"ALYqaps2aomo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1703500435915,"user_tz":480,"elapsed":29425,"user":{"displayName":"Danse Macabre","userId":"12265555024692008343"}},"outputId":"2b5f43c0-4c72-4a30-95cb-853061ce6988"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["# We need to save it to drive because the model is waaay too big to rerun everything\n","# everytime\n","from google.colab import drive\n","\n","# Mount Google Drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"wE62PBoZUno7","executionInfo":{"status":"ok","timestamp":1703500478343,"user_tz":480,"elapsed":42438,"user":{"displayName":"Danse Macabre","userId":"12265555024692008343"}}},"outputs":[],"source":["# Specify the path to save the model\n","model_path = \"/content/gdrive/My Drive/Colab Notebooks/TENSORFLOW/ModelFoodVisionBig/\"\n","\n","model.save(model_path)"]},{"cell_type":"markdown","metadata":{"id":"bto5osBkTygY"},"source":["## Fine-Tunning"]},{"cell_type":"markdown","metadata":{"id":"EwvzyHQ-hQ_o"},"source":["### Loading Model"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"RQOZvubKhQaz","executionInfo":{"status":"ok","timestamp":1703500512192,"user_tz":480,"elapsed":14838,"user":{"displayName":"Danse Macabre","userId":"12265555024692008343"}}},"outputs":[],"source":["# Load model previously saved above\n","loaded_model = tf.keras.models.load_model(\"/content/gdrive/My Drive/Colab Notebooks/TENSORFLOW/ModelFoodVisionBig/\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":59661,"status":"ok","timestamp":1703331902946,"user":{"displayName":"Danse Macabre","userId":"12265555024692008343"},"user_tz":480},"id":"1Ni3lk9Wa3a5","outputId":"740861d7-fcd9-4a5e-9795-c4724eaf4aed"},"outputs":[{"name":"stdout","output_type":"stream","text":["790/790 [==============================] - 59s 66ms/step - loss: 2.4630 - accuracy: 0.5788\n"]},{"data":{"text/plain":["[2.4630420207977295, 0.5788118839263916]"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["loaded_model.evaluate(test_data)"]},{"cell_type":"markdown","metadata":{"id":"zBc9ZkGgVTTU"},"source":["### Setting EarlyStop callback, model checkpoint callback again AND ReduceLROnPlateau\n","\n","The ReduceLROnPlateau callback helps to tune the learning rate for you.\n","\n","Like the ModelCheckpoint and EarlyStopping callbacks, the ReduceLROnPlateau callback montiors a specified metric and when that metric stops improving, it reduces the learning rate by a specified factor (e.g. divides the learning rate by 10).\n","\n","**But why lower the learning rate?**\n","\n","Imagine having a coin at the back of the couch and you're trying to grab with your fingers. Now think of the learning rate as the size of the movements your hand makes towards the coin, the closer you get, the smaller you want your hand movements to be, otherwise the coin will be lost.\n","\n","Our model's ideal performance is the equivalent of grabbing the coin. So as training goes on and our model gets closer and closer to it's ideal performance (also called **convergence**), we want the amount it learns to be less and less.\n","\n","To do this we'll create an instance of the *ReduceLROnPlateau* callback to monitor the validation loss just like the EarlyStopping callback. Once the validation loss stops improving for two or more epochs, we'll reduce the learning rate by a factor of 5 (e.g. 0.001 to 0.0002).\n","\n","And to make sure the learning rate doesn't get too low (and potentially result in our model learning nothing), we'll set the minimum learning rate to 1e-7."]},{"cell_type":"code","execution_count":18,"metadata":{"id":"Zh8inboBUoUc","executionInfo":{"status":"ok","timestamp":1703500478347,"user_tz":480,"elapsed":25,"user":{"displayName":"Danse Macabre","userId":"12265555024692008343"}}},"outputs":[],"source":["# This will stop training if model's val_loss doesn't improve for 3 epochs\n","early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\",\n","                                                  patience=3)\n","\n","# This time, we will save best model during fine-tuning (monitor val_loss while\n","# training and save the best model (lowest val_loss))\n","checkpoint_path2 = \"model_checkpoints2/cpFineTunning.ckpt\" # saving weights requires \".ckpt\" extension\n","model_checkpoint2 = tf.keras.callbacks.ModelCheckpoint(checkpoint_path2,\n","                                                      monitor=\"val_loss\",\n","                                                      save_best_only=True)\n","\n","# Creating learning rate reduction callback\n","reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\",\n","                                                 factor=0.2, # multiply the learning rate by 0.2 (reduce by 5x)\n","                                                 patience=2,\n","                                                 verbose=1, # print out when learning rate goes down\n","                                                 min_lr=1e-7)"]},{"cell_type":"markdown","metadata":{"id":"Zelr0o-9qb7Z"},"source":[]},{"cell_type":"markdown","metadata":{"id":"K1AtnElUZ7sQ"},"source":["### Unfreezing ALL layers, Compiling and Fitting"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"cs9rw7rzaoA3","executionInfo":{"status":"ok","timestamp":1703500525948,"user_tz":480,"elapsed":310,"user":{"displayName":"Danse Macabre","userId":"12265555024692008343"}}},"outputs":[],"source":["# Are any of the layers in our model frozen?\n","for layer in loaded_model.layers:\n","    layer.trainable = True # set all layers to trainable"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"8R8qGqxtbBGa","executionInfo":{"status":"ok","timestamp":1703500527963,"user_tz":480,"elapsed":309,"user":{"displayName":"Danse Macabre","userId":"12265555024692008343"}}},"outputs":[],"source":["loaded_model.compile(loss=\"sparse_categorical_crossentropy\",\n","              optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n","              metrics=[\"accuracy\"])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":580,"status":"ok","timestamp":1703334625079,"user":{"displayName":"Danse Macabre","userId":"12265555024692008343"},"user_tz":480},"id":"SdY0pReKgvVb","outputId":"f54d2d03-ad84-4c5f-f1ec-424754acfdc1"},"outputs":[{"data":{"text/plain":["<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7b586b593c40>"]},"execution_count":43,"metadata":{},"output_type":"execute_result"}],"source":["# And now to make sure it starts at the same checkpoint, we can load the checkpointed\n","# weights from checkpoint_path:\n","model.load_weights(checkpoint_path)"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":449422,"status":"ok","timestamp":1703502438312,"user":{"displayName":"Danse Macabre","userId":"12265555024692008343"},"user_tz":480},"id":"9-IRhoEMd4kL","outputId":"7f3a5334-7265-4801-c500-ebc15ff0ea71"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/100\n","2368/2368 [==============================] - 426s 155ms/step - loss: 0.8680 - accuracy: 0.7653 - val_loss: 0.7351 - val_accuracy: 0.7905 - lr: 1.0000e-04\n","Epoch 2/100\n","2368/2368 [==============================] - 370s 155ms/step - loss: 0.5132 - accuracy: 0.8577 - val_loss: 0.6985 - val_accuracy: 0.8046 - lr: 1.0000e-04\n","Epoch 3/100\n","2368/2368 [==============================] - 336s 141ms/step - loss: 0.2648 - accuracy: 0.9239 - val_loss: 0.8391 - val_accuracy: 0.7895 - lr: 1.0000e-04\n","Epoch 4/100\n","2368/2368 [==============================] - ETA: 0s - loss: 0.1336 - accuracy: 0.9604\n","Epoch 4: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n","2368/2368 [==============================] - 329s 138ms/step - loss: 0.1336 - accuracy: 0.9604 - val_loss: 0.9041 - val_accuracy: 0.8053 - lr: 1.0000e-04\n","Epoch 5/100\n","2368/2368 [==============================] - 331s 139ms/step - loss: 0.0259 - accuracy: 0.9941 - val_loss: 0.9352 - val_accuracy: 0.8252 - lr: 2.0000e-05\n"]}],"source":["history1_fine_tunned = loaded_model.fit(train_data,\n","                                        epochs=100,\n","                                        steps_per_epoch = len(train_data),\n","                                        validation_data = test_data,\n","                                        validation_steps = int(0.15 * len(test_data)),\n","                                        callbacks=[early_stopping, reduce_lr, model_checkpoint2])"]},{"cell_type":"markdown","metadata":{"id":"71kwf4m7kI-p"},"source":["### Evaluating the model"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":84057,"status":"ok","timestamp":1703502536209,"user":{"displayName":"Danse Macabre","userId":"12265555024692008343"},"user_tz":480},"id":"OrQLUdApkSwA","outputId":"8e125de4-9828-4da6-9529-59682125b7d7"},"outputs":[{"output_type":"stream","name":"stdout","text":["790/790 [==============================] - 60s 76ms/step - loss: 0.9304 - accuracy: 0.8158\n"]},{"output_type":"execute_result","data":{"text/plain":["[0.930356502532959, 0.8158019781112671]"]},"metadata":{},"execution_count":25}],"source":["loaded_model.evaluate(test_data)"]},{"cell_type":"markdown","source":["## Saving the model fine-tunned"],"metadata":{"id":"bTky4Y-dm4LX"}},{"cell_type":"code","source":["# We need to save it to drive because the model is waaay too big to rerun everything\n","# everytime\n","from google.colab import drive\n","\n","# Mount Google Drive\n","drive.mount('/content/gdrive')\n","\n","# Specify the path to save the model\n","model_path = \"/content/gdrive/My Drive/Colab Notebooks/TENSORFLOW/ModelFoodVisionBigFT/\"\n","\n","model.save(model_path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5Fhyd0rVm6ym","executionInfo":{"status":"ok","timestamp":1703502632107,"user_tz":480,"elapsed":38097,"user":{"displayName":"Danse Macabre","userId":"12265555024692008343"}},"outputId":"ba49b6db-a868-4344-b99e-6cdbfe453378"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"]}]},{"cell_type":"markdown","metadata":{"id":"F3AP3Md4RI3J"},"source":["# Extra Exercises"]},{"cell_type":"markdown","metadata":{"id":"-NYKrEbbRSRL"},"source":["## Exercise 1 Use the same evaluation techniques on the large-scale Food Vision model as you did in the previous notebook (Transfer Learning Part 3: Scaling up):\n","\n","More specifically, it would be good to see:\n","\n","* A confusion matrix between all of the model's predictions and true labels.\n","* A graph showing the f1-scores of each class.\n","* A visualization of the model making predictions on various images and\n","  comparing the predictions to the ground truth.\n","    * For example, plot a sample image from the test dataset and have the title\n","      of the plot show the prediction, the prediction probability and the ground truth label."]},{"cell_type":"code","source":["# prompt: A confusion matrix between all of the model's predictions and true labels\n","\n","from sklearn.metrics import confusion_matrix\n","import numpy as np\n","\n","\n","# Get true labels\n","\n","\n","# Get predictions on the test data\n","predictions = loaded_model.predict(test_data)\n","\n","# Create confusion matrix\n","cm = confusion_matrix(true_labels, predictions)\n","# Plot confusion matrix\n","plt.imshow(cm, cmap=plt.cm.Blues)\n","plt.xlabel('Predicted label')\n","plt.ylabel('True label')\n","plt.colorbar()\n","plt.show()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":481},"id":"wBa2ji_Hx5w2","executionInfo":{"status":"error","timestamp":1703505260388,"user_tz":480,"elapsed":47496,"user":{"displayName":"Danse Macabre","userId":"12265555024692008343"}},"outputId":"ed00727a-8f17-41a1-8517-5ae898f5e544"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["\r  1/790 [..............................] - ETA: 2:10"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-31-7395214a9c2d>:12: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  labels = np.array(labels)\n"]},{"output_type":"stream","name":"stdout","text":[" 25/790 [..............................] - ETA: 1:00"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-31-7395214a9c2d>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Get predictions on the test data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloaded_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Create confusion matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   2653\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2654\u001b[0m                         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2655\u001b[0;31m                         \u001b[0mtmp_batch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2656\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2657\u001b[0m                             \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 832\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    875\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 877\u001b[0;31m       results = tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    878\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m       )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1321\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1322\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1487\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"m53-42zKR58D"},"source":["## Exercise 2: Take 3 of your own photos of food and use the Food Vision model to make predictions on them. How does it go?"]},{"cell_type":"markdown","metadata":{"id":"XVxdU4CxSEPA"},"source":["## Exercise 3: Retrain the model (feature extraction and fine-tuning) we trained in this notebook, except this time use EfficientNetB4 as the base model instead of EfficientNetB0. Do you notice an improvement in performance? Does it take longer to train? Are there any tradeoffs to consider?"]},{"cell_type":"markdown","metadata":{"id":"kyNTfl84SNmA"},"source":["## Exercise 4: Name one important benefit of mixed precision training, how does this benefit take place?"]},{"cell_type":"markdown","metadata":{"id":"KkHu0QCtSU0g"},"source":["#"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyOxMxtXouVq/7R6LNe/MTzd"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}