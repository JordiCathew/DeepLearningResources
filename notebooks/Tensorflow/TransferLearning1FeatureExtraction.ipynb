{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","toc_visible":true,"authorship_tag":"ABX9TyM3VCsuWYiFQUflhdEj/Zli"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Exercises transfer learning feature extraction"],"metadata":{"id":"K5yziVoVrwbQ"}},{"cell_type":"code","source":["# Import dependencies\n","import tensorflow as tf\n","import tensorflow_hub as hub\n","from tensorflow.keras import layers\n","import zipfile\n","import os\n","import matplotlib.pyplot as plt\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","import datetime"],"metadata":{"id":"5NbeZbAV3g-7","executionInfo":{"status":"ok","timestamp":1699009097148,"user_tz":420,"elapsed":3035,"user":{"displayName":"Danse Macabre","userId":"12265555024692008343"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["## Exercise 1: Build and fit a model using the same data of the class but with the MobileNetV2 architecture feature extraction (mobilenet_v2_100_224/feature_vector) from TensorFlow Hub, how does it perform compared to our other models?"],"metadata":{"id":"0xWYf82rsD1x"}},{"cell_type":"markdown","source":["### Importing the data"],"metadata":{"id":"9Rb61LSxt5xT"}},{"cell_type":"code","source":["# Download data\n","!wget https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_10_percent.zip\n","\n","# Unzip the downloaded file\n","zip_ref = zipfile.ZipFile(\"10_food_classes_10_percent.zip\", \"r\")\n","zip_ref.extractall()\n","zip_ref.close()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xWmmtUBSsVah","executionInfo":{"status":"ok","timestamp":1699008501663,"user_tz":420,"elapsed":11407,"user":{"displayName":"Danse Macabre","userId":"12265555024692008343"}},"outputId":"57b729d2-ecf9-427d-86a6-21aa9a196630"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2023-11-03 10:48:08--  https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_10_percent.zip\n","Resolving storage.googleapis.com (storage.googleapis.com)... 108.177.125.207, 142.251.8.207, 142.251.170.207, ...\n","Connecting to storage.googleapis.com (storage.googleapis.com)|108.177.125.207|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 168546183 (161M) [application/zip]\n","Saving to: ‘10_food_classes_10_percent.zip’\n","\n","10_food_classes_10_ 100%[===================>] 160.74M  30.8MB/s    in 6.0s    \n","\n","2023-11-03 10:48:14 (26.8 MB/s) - ‘10_food_classes_10_percent.zip’ saved [168546183/168546183]\n","\n"]}]},{"cell_type":"code","source":["# How many images in each folder?\n","# Walk through 10 percent data directory and list number of files\n","for dirpath, dirnames, filenames in os.walk(\"10_food_classes_10_percent\"):\n","    print(f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NDp0KBuNu8US","executionInfo":{"status":"ok","timestamp":1699008501666,"user_tz":420,"elapsed":38,"user":{"displayName":"Danse Macabre","userId":"12265555024692008343"}},"outputId":"cc98c89e-25f3-4eb8-bddf-d865ce2a010f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["There are 2 directories and 0 images in '10_food_classes_10_percent'.\n","There are 10 directories and 0 images in '10_food_classes_10_percent/test'.\n","There are 0 directories and 250 images in '10_food_classes_10_percent/test/fried_rice'.\n","There are 0 directories and 250 images in '10_food_classes_10_percent/test/ice_cream'.\n","There are 0 directories and 250 images in '10_food_classes_10_percent/test/chicken_wings'.\n","There are 0 directories and 250 images in '10_food_classes_10_percent/test/sushi'.\n","There are 0 directories and 250 images in '10_food_classes_10_percent/test/grilled_salmon'.\n","There are 0 directories and 250 images in '10_food_classes_10_percent/test/pizza'.\n","There are 0 directories and 250 images in '10_food_classes_10_percent/test/ramen'.\n","There are 0 directories and 250 images in '10_food_classes_10_percent/test/hamburger'.\n","There are 0 directories and 250 images in '10_food_classes_10_percent/test/chicken_curry'.\n","There are 0 directories and 250 images in '10_food_classes_10_percent/test/steak'.\n","There are 10 directories and 0 images in '10_food_classes_10_percent/train'.\n","There are 0 directories and 75 images in '10_food_classes_10_percent/train/fried_rice'.\n","There are 0 directories and 75 images in '10_food_classes_10_percent/train/ice_cream'.\n","There are 0 directories and 75 images in '10_food_classes_10_percent/train/chicken_wings'.\n","There are 0 directories and 75 images in '10_food_classes_10_percent/train/sushi'.\n","There are 0 directories and 75 images in '10_food_classes_10_percent/train/grilled_salmon'.\n","There are 0 directories and 75 images in '10_food_classes_10_percent/train/pizza'.\n","There are 0 directories and 75 images in '10_food_classes_10_percent/train/ramen'.\n","There are 0 directories and 75 images in '10_food_classes_10_percent/train/hamburger'.\n","There are 0 directories and 75 images in '10_food_classes_10_percent/train/chicken_curry'.\n","There are 0 directories and 75 images in '10_food_classes_10_percent/train/steak'.\n"]}]},{"cell_type":"markdown","source":["### Preparing the data (creating data loaders)"],"metadata":{"id":"QiWUL-7yvkI8"}},{"cell_type":"code","source":["IMAGE_SHAPE = (224, 224)\n","BATCH_SIZE = 32\n","\n","train_dir = \"10_food_classes_10_percent/train/\"\n","test_dir = \"10_food_classes_10_percent/test/\"\n","\n","train_datagen = ImageDataGenerator(rescale=1/255.)\n","test_datagen = ImageDataGenerator(rescale=1/255.)\n","\n","train_data_10_percent = train_datagen.flow_from_directory(train_dir, target_size=IMAGE_SHAPE,\n","                                                          batch_size=BATCH_SIZE, class_mode=\"categorical\")\n","\n","test_data = test_datagen.flow_from_directory(test_dir, target_size=IMAGE_SHAPE,\n","                                                          batch_size=BATCH_SIZE, class_mode=\"categorical\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3Ah9F05dvoLI","executionInfo":{"status":"ok","timestamp":1699008501667,"user_tz":420,"elapsed":32,"user":{"displayName":"Danse Macabre","userId":"12265555024692008343"}},"outputId":"db58a61d-21d1-4fe3-c0c1-9d158ddd6faf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 750 images belonging to 10 classes.\n","Found 2500 images belonging to 10 classes.\n"]}]},{"cell_type":"markdown","source":["### Setting up callbacks"],"metadata":{"id":"Advbq0ZtyaRo"}},{"cell_type":"code","source":["# Create TensorBoard callback (functionized beause we need to create a new one for each model)\n","def create_tensorboard_callback(dir_name, experiment_name):\n","    log_dir = dir_name + \"/\" + experiment_name + \"/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n","    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n","    print(f\"Saving TensorBoard log files to: {log_dir}\")\n","    return tensorboard_callback"],"metadata":{"id":"iwAoiUv2yZDn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Creating model"],"metadata":{"id":"7eV1hK5x2O9o"}},{"cell_type":"code","source":["# Let's compare the following two models\n","resnet_url =\"https://tfhub.dev/google/imagenet/resnet_v2_50/feature_vector/4\"\n","efficientnet_url =\"https://tfhub.dev/tensorflow/efficientnet/b0/feature-vector/1\"\n","\n","# IMAGE_SHAPE + (3,)  = (224, 224, 3)\n","\n","# Let's make a create_model() function to create a model from a URL\n","def create_model(model_url, num_classes=10):\n","    \"\"\"\n","    Takes a TensorFlow Hub URL and creates a Keras Sequential model with\n","    it.\n","    Args:\n","        model_url (str): A TensorFlow Hub feature extraction URL.\n","        num_classes (int): Number of output neurons in the output layer,\n","        should be equal to number of target classes, default 10.\n","    Returns:\n","      An uncompiled Keras Sequential model with model_url as feature\n","      extractor layer and Dense output layer with num_classes output neurons.\n","    \"\"\"\n","    # Download the pretrained model and save it as a Keras layer\n","    feature_extractor_layer = hub.KerasLayer(model_url, trainable=False, # freeze the already learned patterns\n","    name=\"feature_extraction_layer\", input_shape=IMAGE_SHAPE+(3,))\n","\n","    # Create our own model\n","    model = tf.keras.Sequential([\n","    feature_extractor_layer,\n","    layers.Dense(num_classes, activation=\"softmax\",\n","    name=\"output_layer\")\n","    ])\n","\n","    return model"],"metadata":{"id":"WxWURo_-zThU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create mobilenet model\n","mobilenet_model = create_model(\"https://www.kaggle.com/models/google/mobilenet-v3/frameworks/TensorFlow2/variations/large-075-224-classification/versions/1\",\n","                               num_classes=train_data_10_percent.num_classes)\n","\n","# Compile our mobilenet model\n","mobilenet_model.compile(loss=\"categorical_crossentropy\", optimizer=tf.keras.optimizers.Adam(),\n","                        metrics=[\"accuracy\"])\n","\n","# Let's fit our ResNet model to the data (10 percent of 10 classes)\n","mobilenet_history = mobilenet_model.fit(train_data_10_percent, epochs=5,\n","                                     steps_per_epoch=len(train_data_10_percent),\n","                                     validation_data=test_data,\n","                                     validation_steps=len(test_data),\n","                                     callbacks=[create_tensorboard_callback(dir_name=\"tensorflow_hub\",\n","                                                                            experiment_name=\"mobilenetV3\")])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MCIPFaveHaLx","executionInfo":{"status":"ok","timestamp":1699008574281,"user_tz":420,"elapsed":72639,"user":{"displayName":"Danse Macabre","userId":"12265555024692008343"}},"outputId":"cf9e6fce-ecbc-406e-fce4-f0f6fe0c975e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Saving TensorBoard log files to: tensorflow_hub/mobilenetV3/20231103-104829\n","Epoch 1/5\n","24/24 [==============================] - 23s 451ms/step - loss: 2.2270 - accuracy: 0.3187 - val_loss: 1.2052 - val_accuracy: 0.5972\n","Epoch 2/5\n","24/24 [==============================] - 8s 363ms/step - loss: 0.9439 - accuracy: 0.6960 - val_loss: 0.7510 - val_accuracy: 0.7504\n","Epoch 3/5\n","24/24 [==============================] - 9s 378ms/step - loss: 0.6180 - accuracy: 0.7987 - val_loss: 0.6194 - val_accuracy: 0.7916\n","Epoch 4/5\n","24/24 [==============================] - 9s 393ms/step - loss: 0.4590 - accuracy: 0.8600 - val_loss: 0.5865 - val_accuracy: 0.8052\n","Epoch 5/5\n","24/24 [==============================] - 10s 418ms/step - loss: 0.3602 - accuracy: 0.9013 - val_loss: 0.5634 - val_accuracy: 0.8140\n"]}]},{"cell_type":"code","source":["# Plots our loss curves... Tidbit: you could put a function like this into a\n","# script called \"helper.py\" and import it when you need it...\n","\n","# Plot the validation and training curves\n","def plot_loss_curves(history):\n","    \"\"\"\n","    Returns separate loss curves for training and validation metrics.\n","    Args:\n","      history: TensorFlow History object.\n","    Returns:\n","      Plots of training/validation loss and accuracy metrics.\n","    \"\"\"\n","    loss = history.history[\"loss\"]\n","    val_loss = history.history[\"val_loss\"]\n","    accuracy = history.history[\"accuracy\"]\n","    val_accuracy = history.history[\"val_accuracy\"]\n","    epochs = range(len(history.history[\"loss\"]))\n","    # Plot loss\n","    plt.plot(epochs, loss, label=\"training_loss\")\n","    plt.plot(epochs, val_loss, label=\"val_loss\")\n","    plt.title(\"Loss\")\n","    plt.xlabel(\"Epochs\")\n","    plt.legend()\n","    # Plot accuracy\n","    plt.figure()\n","    plt.plot(epochs, accuracy, label=\"training_accuracy\")\n","    plt.plot(epochs, val_accuracy, label=\"val_accuracy\")\n","    plt.title(\"Accuracy\")\n","    plt.xlabel(\"Epochs\")\n","    plt.legend();\n"],"metadata":{"id":"ruXksS30NIjH","executionInfo":{"status":"ok","timestamp":1699009105389,"user_tz":420,"elapsed":296,"user":{"displayName":"Danse Macabre","userId":"12265555024692008343"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["plot_loss_curves(mobilenet_history)"],"metadata":{"id":"9KeLvhKW1BAx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Exercise 2: Name 3 different image classification models on TensorFlow Hub that we haven't used."],"metadata":{"id":"SWqhcrwksK8n"}},{"cell_type":"markdown","source":["1. inception_v3\n","2. Inception ResNet\n","3. Inception family of models\n","4. NASNet models\n","5. Some of the ResNet versions of models\n","\n"],"metadata":{"id":"xObFjWXoPir4"}},{"cell_type":"markdown","source":["## Exercise 3: Build a model to classify images of two different things you've taken photos of.\n","\n","*   You can use any feature extraction layer from TensorFlow Hub you like for this.\n","* You should aim to have at least 10 images of each class, for example to build a fridge versus oven classifier, you'll want 10 images of fridges and 10 images of ovens."],"metadata":{"id":"D1KRZv7AsT-P"}},{"cell_type":"markdown","source":["### Importing the data"],"metadata":{"id":"qjIuz-O5yRlO"}},{"cell_type":"code","source":["# We upload our custom folder and we extract it\n","zip_ref = zipfile.ZipFile(\"shoesAndPens.zip\", \"r\")\n","zip_ref.extractall()\n","zip_ref.close()\n","\n","# Walk through data directory and list number of files\n","for dirpath, dirnames, filenames in os.walk(\"shoesAndPens\"):\n","    print(f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H2WauEppyrok","executionInfo":{"status":"ok","timestamp":1699009489428,"user_tz":420,"elapsed":819,"user":{"displayName":"Danse Macabre","userId":"12265555024692008343"}},"outputId":"173a7588-1c5a-4846-9b4b-fbc275313bf3"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["There are 2 directories and 0 images in 'shoesAndPens'.\n","There are 2 directories and 0 images in 'shoesAndPens/test'.\n","There are 0 directories and 2 images in 'shoesAndPens/test/pens'.\n","There are 0 directories and 2 images in 'shoesAndPens/test/shoes'.\n","There are 2 directories and 0 images in 'shoesAndPens/train'.\n","There are 0 directories and 10 images in 'shoesAndPens/train/pens'.\n","There are 0 directories and 10 images in 'shoesAndPens/train/shoes'.\n"]}]},{"cell_type":"markdown","source":["### Preparing the data (creating data loaders)"],"metadata":{"id":"AzSBLLPJyUFl"}},{"cell_type":"code","source":["train_dir = \"shoesAndPens/train/\"\n","test_dir = \"shoesAndPens/test/\"\n","\n","train_datagen = ImageDataGenerator(rescale=1/255.)\n","test_datagen = ImageDataGenerator(rescale=1/255.)\n","\n","train_data = train_datagen.flow_from_directory(train_dir, target_size=(224, 224),\n","                                                          batch_size=8, class_mode=\"binary\")\n","\n","test_data = test_datagen.flow_from_directory(test_dir, target_size=(224, 224),\n","                                                          batch_size=8, class_mode=\"binary\")"],"metadata":{"id":"rxeHepIRyaN6","executionInfo":{"status":"ok","timestamp":1699010169493,"user_tz":420,"elapsed":306,"user":{"displayName":"Danse Macabre","userId":"12265555024692008343"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"576f2d26-f325-4d35-acc7-c7db5575e35b"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 20 images belonging to 2 classes.\n","Found 4 images belonging to 2 classes.\n"]}]},{"cell_type":"markdown","source":["### Creating model"],"metadata":{"id":"7tNNyusZyeef"}},{"cell_type":"code","source":["efficientnet_feature_extraction_layer = hub.KerasLayer(\"https://www.kaggle.com/models/google/efficientnet-v2/frameworks/TensorFlow2/variations/imagenet1k-b0-classification/versions/2\",\n","                                                       trainable=False, input_shape=(224, 224) + (3,))\n"],"metadata":{"id":"EU3kIqxOyhcB","executionInfo":{"status":"ok","timestamp":1699011019703,"user_tz":420,"elapsed":9321,"user":{"displayName":"Danse Macabre","userId":"12265555024692008343"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["# Building a Sequential model with our mobilenet feature extraction layer for our custom data\n","custom_data_model = tf.keras.Sequential([\n","  efficientnet_feature_extraction_layer,\n","  layers.Dense(1, activation= 'sigmoid', name ='output_layer')\n","])\n","\n","# Printing the summarr of the model\n","custom_data_model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wUyQw86L8tFs","executionInfo":{"status":"ok","timestamp":1699011144562,"user_tz":420,"elapsed":1157,"user":{"displayName":"Danse Macabre","userId":"12265555024692008343"}},"outputId":"71bd6439-855c-439b-9b3c-1a5f55d782ca"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," keras_layer (KerasLayer)    (None, 1000)              7200312   \n","                                                                 \n"," output_layer (Dense)        (None, 1)                 1001      \n","                                                                 \n","=================================================================\n","Total params: 7201313 (27.47 MB)\n","Trainable params: 1001 (3.91 KB)\n","Non-trainable params: 7200312 (27.47 MB)\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["# Compiling the model\n","custom_data_model.compile(loss = tf.keras.losses.BinaryCrossentropy() ,\n","                          optimizer = tf.keras.optimizers.Adam(learning_rate= 0.001) ,\n","                          metrics = ['accuracy'])\n","\n","\n","# Fitting the model\n","custom_model_history = custom_data_model.fit(train_data,\n","                      epochs=5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HWrvjb1O9Xv6","executionInfo":{"status":"ok","timestamp":1699011260622,"user_tz":420,"elapsed":8508,"user":{"displayName":"Danse Macabre","userId":"12265555024692008343"}},"outputId":"7d026dbe-eec0-476f-aea0-69bf286c7425"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","3/3 [==============================] - 3s 375ms/step - loss: 0.0186 - accuracy: 1.0000\n","Epoch 2/5\n","3/3 [==============================] - 1s 417ms/step - loss: 0.0097 - accuracy: 1.0000\n","Epoch 3/5\n","3/3 [==============================] - 1s 419ms/step - loss: 0.0058 - accuracy: 1.0000\n","Epoch 4/5\n","3/3 [==============================] - 1s 336ms/step - loss: 0.0035 - accuracy: 1.0000\n","Epoch 5/5\n","3/3 [==============================] - 1s 430ms/step - loss: 0.0020 - accuracy: 1.0000\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"vVRrm8eH-g9g"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Exercise 4: What is the current best performing model on ImageNet?\n","Hint: you might want to check sotabench.com for this."],"metadata":{"id":"RCGSafGYtF8B"}},{"cell_type":"markdown","source":["It seems like BASIC-L (Lion, fine-tuned) and efficient-netL2 is the state-of-the-art model now performing really good in Image Classification task.\n","\n","Top 1% accuracy --> 91.1% (BASIC-L)\n","Top 5% accuracy --> 98.9% (efficient-netL2)\n","\n","To know more:\n","\n","https://paperswithcode.com/sota/image-classification-on-imagenet"],"metadata":{"id":"h41OeleHRU9k"}}]}